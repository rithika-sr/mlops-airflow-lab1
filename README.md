#  MLOps Lab 1: Airflow + K-Means Workflow


## ğŸ§© Lab Overview
This lab demonstrates how to **orchestrate an end-to-end machine learning workflow** using **Apache Airflow**, from data loading and preprocessing to model training and simulated serving.  
It automates a **K-Means clustering pipeline**, showcasing key principles of **reproducibility, modularity, and scheduling** within MLOps.


## âš™ï¸ Tech Stack
| Category | Tools Used |
|-----------|-------------|
| Workflow Orchestration | Apache Airflow |
| Machine Learning | Scikit-Learn |
| Data Processing | Pandas |
| Model Persistence | Pickle |
| Visualization / CLI | Airflow Web UI |
| Language | Python 3.x |



## ğŸ§  Workflow Summary (Airflow DAG)
**DAG Name:** `Airflow_Lab1_Rithika`  
**Description:** K-Means Clustering Workflow + Model Save + Serving Simulation  

### ğŸ“Š Task Flow
1. **`load_data_task`** â€“ Creates a small synthetic dataset.  
2. **`data_preprocessing_task`** â€“ Performs normalization.  
3. **`build_save_model_task`** â€“ Trains a K-Means model, finds the optimal *k* using the elbow method, and saves it as a `.pkl` file.  
4. **`serve_model_task`** â€“ Simulates FastAPI serving (mock endpoint).  

âœ… Each task uses **`PythonOperator`** and passes objects via **XCom (Pickle serialization)**.  
âœ… Workflow ensures modular design, making each task independently testable and reusable.


## ğŸ—‚ï¸ Project Structure


mlops-airflow-lab1/
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ airflow_lab1_dag.py           # Main DAG file (this project)
â”‚
â”œâ”€â”€ logs/                             # Airflow-generated logs
â”‚   â””â”€â”€ dag_processor_manager/
â”‚
â”œâ”€â”€ working_data/
â”‚   â”œâ”€â”€ iris.csv                      # Sample dataset
â”‚   â””â”€â”€ model_rithika.pkl             # Saved model artifact
â”‚
â”œâ”€â”€ .gitignore                        # Excludes logs & temp files
â””â”€â”€ README.md                         # Project documentation




## âš¡ How to Run Locally

### 1ï¸âƒ£ Start Airflow Environment
```bash
docker-compose up airflow-init
docker-compose up
````

### 2ï¸âƒ£ Access Airflow UI

Open your browser at â†’ **[http://localhost:8080](http://localhost:8080)**
Login (default):

```
Username: airflow2
Password: airflow2
```

### 3ï¸âƒ£ Trigger the DAG

* Go to **DAGs â†’ Airflow_Lab1_Rithika**
* Turn the toggle **ON**
* Click â–¶ï¸ **Trigger DAG**
* Monitor tasks under the **Graph** view



## ğŸ“¦ Outputs

* Trained **K-Means model** â†’ `model_rithika.pkl`
* Airflow logs under `/logs/`
* Clean workflow orchestration verified through DAG success states



## ğŸ¯ Key Learnings

* Building reproducible ML workflows with Airflow
* Managing data and model dependencies via **XComs**
* Automating model lifecycle steps (load â†’ preprocess â†’ train â†’ serve)
* Incorporating modular, maintainable Python scripts in DAGs


```
